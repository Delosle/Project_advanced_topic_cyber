{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f93e72-433c-4f57-94b3-0fe982b1bcad",
   "metadata": {},
   "source": [
    "This notebook implements the complete system for:\n",
    "\n",
    "Activity 1: PIN code entry (4 digits: 0‚Äì9)\n",
    "Activity 2: Application detection\n",
    "\n",
    "Architecture:\n",
    "    DT + SVM: Determines the activity (PIN or Application) from 19 features\n",
    "    PIN CNN: If the activity = PIN, classifies which digit (0‚Äì9)\n",
    "    Application CNN: If the activity = Application, classifies which app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4f6b2-7548-46e0-b112-365ceecf8555",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "Imports and GPU configuration\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a7e66a-b1b9-435e-853d-678e7c57f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 10:54:46.067251: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-19 10:54:47.361002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-19 10:54:51.011948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    CONFIGURATION SYST√àME\n",
      "======================================================================\n",
      "TensorFlow version: 2.20.0\n",
      "Nombre de GPUs: 1\n",
      "‚úÖ GPU configur√©: /physical_device:GPU:0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Sklearn pour DT + SVM\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# SciPy pour le pr√©traitement des signaux\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "\n",
    "# Librosa pour MFCC\n",
    "import librosa\n",
    "\n",
    "# TensorFlow/Keras pour CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Configuration matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"                    CONFIGURATION SYST√àME\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Nombre de GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Configuration GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU configur√©: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå Erreur GPU: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CPU utilis√© (pas de GPU d√©tect√©)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95879-fc93-491e-a8ce-c5b1ea41f488",
   "metadata": {},
   "source": [
    "-------------------\n",
    "Data pre-treatement functions\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04989bc-2f85-410f-b482-d300f3535835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "     D√âFINITION DES FONCTIONS DE PR√âTRAITEMENT\n",
      "======================================================================\n",
      "‚úÖ Fonctions de pr√©traitement d√©finies:\n",
      "   - apply_lowpass_filter() : Filtre passe-bas 500 Hz\n",
      "   - calculate_sliding_window_sums() : Eq. (14) du papier\n",
      "   - detect_activity_boundaries() : D√©tection d√©but/fin\n",
      "   - preprocess_voltage_signal() : Pipeline complet\n",
      "   - extract_19_features() : 6 stat + 13 MFCC\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     D√âFINITION DES FONCTIONS DE PR√âTRAITEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def apply_lowpass_filter(raw_signal, sampling_freq=5000, cutoff_freq=500):\n",
    "    \"\"\"\n",
    "    Applique un filtre passe-bas Butterworth d'ordre 4\n",
    "    Selon Section 5.2 du papier\n",
    "    \n",
    "    Args:\n",
    "        raw_signal: signal brut\n",
    "        sampling_freq: fr√©quence d'√©chantillonnage (5 KHz par d√©faut)\n",
    "        cutoff_freq: fr√©quence de coupure (500 Hz)\n",
    "    \"\"\"\n",
    "    nyquist = sampling_freq / 2\n",
    "    normalized_cutoff = cutoff_freq / nyquist\n",
    "    \n",
    "    # Filtre Butterworth d'ordre 4\n",
    "    b, a = scipy_signal.butter(4, normalized_cutoff, btype='low')\n",
    "    filtered_signal = scipy_signal.filtfilt(b, a, raw_signal)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def calculate_sliding_window_sums(signal_data, window_size=50):\n",
    "    \"\"\"\n",
    "    Calcule les sommes dans une fen√™tre glissante\n",
    "    Impl√©mente l'√©quation (14) du papier:\n",
    "    S = sum(|v_{i+1} - v_i|) pour i dans la fen√™tre\n",
    "    \n",
    "    Args:\n",
    "        signal_data: signal filtr√©\n",
    "        window_size: taille de la fen√™tre (Nw dans le papier)\n",
    "    \n",
    "    Returns:\n",
    "        array des sommes\n",
    "    \"\"\"\n",
    "    sums = []\n",
    "    for i in range(len(signal_data) - window_size):\n",
    "        window = signal_data[i:i+window_size]\n",
    "        S = np.sum(np.abs(np.diff(window)))  # Eq. (14)\n",
    "        sums.append(S)\n",
    "    \n",
    "    return np.array(sums)\n",
    "\n",
    "\n",
    "def detect_activity_boundaries(sums, n_consecutive=5):\n",
    "    \"\"\"\n",
    "    D√©tecte les points de d√©but et fin des activit√©s\n",
    "    \n",
    "    Args:\n",
    "        sums: array des sommes de la fen√™tre glissante\n",
    "        n_consecutive: nombre de points cons√©cutifs pour confirmer d√©but/fin\n",
    "    \n",
    "    Returns:\n",
    "        list de tuples (start_idx, end_idx)\n",
    "    \"\"\"\n",
    "    # Calculer le seuil\n",
    "    threshold = np.mean(sums) + 2 * np.std(sums)\n",
    "    \n",
    "    boundaries = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(sums) - n_consecutive:\n",
    "        # Chercher un d√©but d'activit√© (n points cons√©cutifs en hausse)\n",
    "        if all(sums[i+j] < sums[i+j+1] for j in range(n_consecutive-1)):\n",
    "            start = i\n",
    "            \n",
    "            # Chercher la fin (n points cons√©cutifs en baisse)\n",
    "            j = start + n_consecutive\n",
    "            while j < len(sums) - n_consecutive:\n",
    "                if all(sums[j+k] > sums[j+k+1] for k in range(n_consecutive-1)):\n",
    "                    end = j + n_consecutive\n",
    "                    boundaries.append((start, end))\n",
    "                    i = end\n",
    "                    break\n",
    "                j += 1\n",
    "            \n",
    "            if j >= len(sums) - n_consecutive:\n",
    "                # Pas de fin trouv√©e, prendre jusqu'√† la fin\n",
    "                boundaries.append((start, len(sums)))\n",
    "                break\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "\n",
    "def preprocess_voltage_signal(raw_signal, sampling_freq=5000, cutoff_freq=500, \n",
    "                               window_size=50, n_consecutive=5):\n",
    "    \"\"\"\n",
    "    Pr√©traitement complet selon Section 5.2 du papier\n",
    "    \n",
    "    √âtapes:\n",
    "    1. Filtrage passe-bas (500 Hz)\n",
    "    2. D√©tection d'activit√© avec fen√™tre glissante (Eq. 14)\n",
    "    3. Segmentation des activit√©s\n",
    "    4. Normalisation (soustraire le minimum)\n",
    "    \n",
    "    Args:\n",
    "        raw_signal: signal de voltage brut\n",
    "        sampling_freq: fr√©quence d'√©chantillonnage (Hz)\n",
    "        cutoff_freq: fr√©quence de coupure du filtre (Hz)\n",
    "        window_size: taille de la fen√™tre glissante\n",
    "        n_consecutive: nombre de points cons√©cutifs pour d√©tecter activit√©\n",
    "    \n",
    "    Returns:\n",
    "        list de segments normalis√©s\n",
    "    \"\"\"\n",
    "    # √âtape 1: Filtrage passe-bas\n",
    "    filtered = apply_lowpass_filter(raw_signal, sampling_freq, cutoff_freq)\n",
    "    \n",
    "    # √âtape 2: Calcul des sommes avec fen√™tre glissante\n",
    "    sums = calculate_sliding_window_sums(filtered, window_size)\n",
    "    \n",
    "    # √âtape 3: D√©tection des fronti√®res d'activit√©s\n",
    "    boundaries = detect_activity_boundaries(sums, n_consecutive)\n",
    "    \n",
    "    # Segmentation et normalisation\n",
    "    segments = []\n",
    "    for start, end in boundaries:\n",
    "        segment = filtered[start:end]\n",
    "        \n",
    "        # √âtape 4: Normalisation (soustraire le minimum)\n",
    "        normalized = segment - np.min(segment)\n",
    "        segments.append(normalized)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "def extract_19_features(signal_segment, sampling_freq=5000):\n",
    "    \"\"\"\n",
    "    Extrait exactement 19 features comme dans le papier (Section 5.2)\n",
    "    \n",
    "    Features extraites:\n",
    "    - 6 features statistiques\n",
    "    - 13 MFCC (Mel Frequency Cepstral Coefficients)\n",
    "    \n",
    "    Args:\n",
    "        signal_segment: segment de signal normalis√©\n",
    "        sampling_freq: fr√©quence d'√©chantillonnage\n",
    "    \n",
    "    Returns:\n",
    "        array de 19 features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # ===== 6 Features Statistiques =====\n",
    "    features.append(np.max(signal_segment))                    # 1. Maximum\n",
    "    features.append(np.std(signal_segment))                    # 2. Standard deviation\n",
    "    features.append(np.var(signal_segment))                    # 3. Variance\n",
    "    features.append(np.mean(signal_segment))                   # 4. Mean\n",
    "    features.append(scipy_entropy(signal_segment + 1e-10))     # 5. Entropy\n",
    "    features.append(len(signal_segment))                       # 6. Length (dur√©e)\n",
    "    \n",
    "    # ===== 13 MFCC =====\n",
    "    # Convertir en float et calculer les MFCC\n",
    "    signal_float = signal_segment.astype(np.float32)\n",
    "    \n",
    "    # Calculer 13 MFCC\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=signal_float,\n",
    "        sr=sampling_freq,\n",
    "        n_mfcc=13\n",
    "    )\n",
    "    \n",
    "    # Prendre la moyenne de chaque coefficient MFCC\n",
    "    mfcc_means = np.mean(mfccs, axis=1)\n",
    "    features.extend(mfcc_means)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions de pr√©traitement d√©finies:\")\n",
    "print(\"   - apply_lowpass_filter() : Filtre passe-bas 500 Hz\")\n",
    "print(\"   - calculate_sliding_window_sums() : Eq. (14) du papier\")\n",
    "print(\"   - detect_activity_boundaries() : D√©tection d√©but/fin\")\n",
    "print(\"   - preprocess_voltage_signal() : Pipeline complet\")\n",
    "print(\"   - extract_19_features() : 6 stat + 13 MFCC\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1006bd-a754-4cde-b126-cd01dc1ee166",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "Charge Data and prepare them\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515a821-10a1-43e4-be68-b560dff8f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FORMAT DES DONN√âES ATTENDU :\n",
    "\n",
    "Pour chaque √©chantillon, vous devez avoir :\n",
    "1. raw_voltage : vecteur de N valeurs (voltages bruts) - sera pr√©trait√©\n",
    "2. activity : 0 = PIN, 1 = Application\n",
    "3. label : \n",
    "   - Si activity=0 (PIN) : chiffre du PIN (0-9)\n",
    "   - Si activity=1 (App) : ID de l'application (0, 1, 2, ...)\n",
    "\n",
    "PARAM√àTRES DE COLLECTE (selon le papier):\n",
    "- Fr√©quence d'√©chantillonnage : 5 KHz\n",
    "- √âtat de la batterie : 20-30%\n",
    "- R√©sistance : 0.33 Ohm sur la ligne d'alimentation\n",
    "\"\"\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# OPTION 1 : DONN√âES SYNTH√âTIQUES (pour tester le syst√®me)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def generate_synthetic_voltage_data(n_samples=2000, base_length=1000, sampling_freq=5000):\n",
    "    \"\"\"\n",
    "    G√©n√®re des donn√©es de voltage synth√©tiques r√©alistes\n",
    "    \n",
    "    Args:\n",
    "        n_samples: nombre total d'√©chantillons\n",
    "        base_length: longueur de base des signaux\n",
    "        sampling_freq: fr√©quence d'√©chantillonnage\n",
    "    \n",
    "    Returns:\n",
    "        raw_voltages, y_activity, y_class\n",
    "    \"\"\"\n",
    "    print(\"üîÑ G√©n√©ration de donn√©es synth√©tiques...\")\n",
    "    \n",
    "    # Moiti√© PIN, moiti√© Applications\n",
    "    n_pin = n_samples // 2\n",
    "    n_app = n_samples - n_pin\n",
    "    \n",
    "    # G√©n√©rer des signaux de voltage bruts (avec bruit et activit√©s)\n",
    "    raw_voltages = []\n",
    "    y_activity = []\n",
    "    y_class = []\n",
    "    \n",
    "    # Signaux PIN (chiffres 0-9)\n",
    "    for i in range(n_pin):\n",
    "        # Bruit de fond\n",
    "        length = np.random.randint(base_length - 200, base_length + 200)\n",
    "        noise = np.random.randn(length) * 0.1\n",
    "        \n",
    "        # Activit√© (pic de voltage)\n",
    "        digit = np.random.randint(0, 10)\n",
    "        activity_pos = length // 2\n",
    "        activity_width = 100\n",
    "        \n",
    "        # Cr√©er un pic caract√©ristique\n",
    "        for j in range(activity_width):\n",
    "            pos = activity_pos + j\n",
    "            if pos < length:\n",
    "                noise[pos] += np.sin(j / activity_width * np.pi) * (digit + 1) * 0.5\n",
    "        \n",
    "        raw_voltages.append(noise)\n",
    "        y_activity.append(0)  # PIN\n",
    "        y_class.append(digit)\n",
    "    \n",
    "    # Signaux Application (apps 0-4)\n",
    "    for i in range(n_app):\n",
    "        # Bruit de fond\n",
    "        length = np.random.randint(base_length - 200, base_length + 200)\n",
    "        noise = np.random.randn(length) * 0.15\n",
    "        \n",
    "        # Activit√© (signature de l'app)\n",
    "        app_id = np.random.randint(0, 5)\n",
    "        activity_pos = length // 2\n",
    "        activity_width = 150\n",
    "        \n",
    "        # Cr√©er une signature diff√©rente pour chaque app\n",
    "        for j in range(activity_width):\n",
    "            pos = activity_pos + j\n",
    "            if pos < length:\n",
    "                noise[pos] += np.cos(j / activity_width * np.pi * (app_id + 1)) * 0.7\n",
    "        \n",
    "        raw_voltages.append(noise)\n",
    "        y_activity.append(1)  # Application\n",
    "        y_class.append(app_id)\n",
    "    \n",
    "    # M√©langer\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    raw_voltages = [raw_voltages[i] for i in indices]\n",
    "    y_activity = np.array([y_activity[i] for i in indices])\n",
    "    y_class = np.array([y_class[i] for i in indices])\n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es g√©n√©r√©es:\")\n",
    "    print(f\"   Total √©chantillons: {n_samples}\")\n",
    "    print(f\"   PIN samples: {(y_activity==0).sum()}\")\n",
    "    print(f\"   App samples: {(y_activity==1).sum()}\")\n",
    "    \n",
    "    return raw_voltages, y_activity, y_class\n",
    "\n",
    "\n",
    "# G√©n√©rer les donn√©es\n",
    "raw_voltages, y_activity, y_class = generate_synthetic_voltage_data(\n",
    "    n_samples=2000,\n",
    "    base_length=1000,\n",
    "    sampling_freq=5000\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Donn√©es brutes g√©n√©r√©es: {len(raw_voltages)} √©chantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393471a-3995-4fdf-a8f1-fa87a4805e12",
   "metadata": {},
   "source": [
    "----------------------\n",
    "Pre-treatment and features extraction\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed95752-17c7-4266-9f8f-50ee3a512b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     PR√âTRAITEMENT DES SIGNAUX ET EXTRACTION DES FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "SAMPLING_FREQ = 5000  # Hz\n",
    "\n",
    "# Conteneurs pour les donn√©es trait√©es\n",
    "X_features_list = []  # Features pour DT+SVM\n",
    "X_sequences_list = []  # S√©quences pour CNN\n",
    "y_activity_processed = []\n",
    "y_class_processed = []\n",
    "\n",
    "print(f\"\\nüîÑ Traitement de {len(raw_voltages)} signaux bruts...\")\n",
    "\n",
    "for idx, raw_signal in enumerate(raw_voltages):\n",
    "    if (idx + 1) % 200 == 0:\n",
    "        print(f\"   Trait√©: {idx + 1}/{len(raw_voltages)}\")\n",
    "    \n",
    "    # Pr√©traiter le signal (filtrage + d√©tection + segmentation)\n",
    "    segments = preprocess_voltage_signal(\n",
    "        raw_signal,\n",
    "        sampling_freq=SAMPLING_FREQ,\n",
    "        cutoff_freq=500,\n",
    "        window_size=50,\n",
    "        n_consecutive=5\n",
    "    )\n",
    "    \n",
    "    # Pour chaque segment d√©tect√©\n",
    "    for segment in segments:\n",
    "        if len(segment) < 50:  # Ignorer les segments trop courts\n",
    "            continue\n",
    "        \n",
    "        # Extraire les 19 features\n",
    "        features = extract_19_features(segment, SAMPLING_FREQ)\n",
    "        \n",
    "        # Normaliser la s√©quence pour le CNN (Z-score)\n",
    "        mean = np.mean(segment)\n",
    "        std = np.std(segment) + 1e-8\n",
    "        sequence_normalized = (segment - mean) / std\n",
    "        \n",
    "        # Stocker\n",
    "        X_features_list.append(features)\n",
    "        X_sequences_list.append(sequence_normalized)\n",
    "        y_activity_processed.append(y_activity[idx])\n",
    "        y_class_processed.append(y_class[idx])\n",
    "\n",
    "# Convertir en arrays\n",
    "X_features = np.array(X_features_list)\n",
    "y_activity = np.array(y_activity_processed)\n",
    "y_class = np.array(y_class_processed)\n",
    "\n",
    "# Pour les s√©quences, on doit les padding √† la m√™me longueur\n",
    "max_length = max(len(seq) for seq in X_sequences_list)\n",
    "print(f\"\\nüìê Longueur maximale des s√©quences: {max_length}\")\n",
    "\n",
    "# Padding ou truncation\n",
    "X_sequences = []\n",
    "for seq in X_sequences_list:\n",
    "    if len(seq) < max_length:\n",
    "        # Padding avec des z√©ros\n",
    "        padded = np.pad(seq, (0, max_length - len(seq)), mode='constant')\n",
    "    else:\n",
    "        # Truncation\n",
    "        padded = seq[:max_length]\n",
    "    X_sequences.append(padded)\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "\n",
    "print(f\"\\n‚úÖ Pr√©traitement termin√©:\")\n",
    "print(f\"   Features shape: {X_features.shape} (19 features par √©chantillon)\")\n",
    "print(f\"   Sequences shape: {X_sequences.shape}\")\n",
    "print(f\"   Activity labels: {y_activity.shape}\")\n",
    "print(f\"   Class labels: {y_class.shape}\")\n",
    "print(f\"   PIN samples: {(y_activity==0).sum()}\")\n",
    "print(f\"   App samples: {(y_activity==1).sum()}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb4879-0655-4d46-bd89-5be4e04e2ea7",
   "metadata": {},
   "source": [
    "--------------\n",
    "Exploration of treated Data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283116c9-bf74-48c9-8d54-2f4ae0d04336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    EXPLORATION DES DONN√âES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Statistiques g√©n√©rales:\")\n",
    "print(f\"   Nombre total d'√©chantillons: {len(X_features)}\")\n",
    "print(f\"   Dimension des features: {X_features.shape[1]} (devrait √™tre 19)\")\n",
    "print(f\"   Longueur des s√©quences: {X_sequences.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüéØ Distribution des activit√©s:\")\n",
    "unique_activities, counts_activities = np.unique(y_activity, return_counts=True)\n",
    "for act, count in zip(unique_activities, counts_activities):\n",
    "    activity_name = \"PIN\" if act == 0 else \"Application\"\n",
    "    print(f\"   {activity_name}: {count} √©chantillons ({count/len(y_activity)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüî¢ Distribution pour PIN (activit√©=0):\")\n",
    "pin_mask = (y_activity == 0)\n",
    "pin_classes = y_class[pin_mask]\n",
    "unique_pins, counts_pins = np.unique(pin_classes, return_counts=True)\n",
    "print(f\"   Classes PIN d√©tect√©es: {unique_pins}\")\n",
    "for pin, count in zip(unique_pins, counts_pins):\n",
    "    print(f\"      Chiffre {pin}: {count} √©chantillons\")\n",
    "\n",
    "print(f\"\\nüì± Distribution pour Applications (activit√©=1):\")\n",
    "app_mask = (y_activity == 1)\n",
    "app_classes = y_class[app_mask]\n",
    "unique_apps, counts_apps = np.unique(app_classes, return_counts=True)\n",
    "print(f\"   Applications d√©tect√©es: {unique_apps}\")\n",
    "for app_id, count in zip(unique_apps, counts_apps):\n",
    "    print(f\"      App {app_id}: {count} √©chantillons\")\n",
    "\n",
    "# Visualisation des 19 features\n",
    "print(f\"\\nüìà Statistiques des features (19 dimensions):\")\n",
    "feature_names = ['Max', 'Std', 'Var', 'Mean', 'Entropy', 'Length'] + \\\n",
    "                [f'MFCC{i+1}' for i in range(13)]\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"   {name:10s}: min={X_features[:, i].min():.3f}, \" +\n",
    "          f\"max={X_features[:, i].max():.3f}, \" +\n",
    "          f\"mean={X_features[:, i].mean():.3f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Distribution activit√©s\n",
    "axes[0, 0].bar(['PIN', 'Application'], counts_activities, color=['#3498db', '#e74c3c'])\n",
    "axes[0, 0].set_title('Distribution des Activit√©s', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Nombre d\\'√©chantillons')\n",
    "for i, (act, count) in enumerate(zip(['PIN', 'App'], counts_activities)):\n",
    "    axes[0, 0].text(i, count, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Distribution PIN\n",
    "axes[0, 1].bar(unique_pins.astype(str), counts_pins, color='#3498db')\n",
    "axes[0, 1].set_title('Distribution des Chiffres PIN', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Chiffre')\n",
    "axes[0, 1].set_ylabel('Nombre d\\'√©chantillons')\n",
    "\n",
    "# Distribution Apps\n",
    "axes[1, 0].bar([f'App {i}' for i in unique_apps], counts_apps, color='#e74c3c')\n",
    "axes[1, 0].set_title('Distribution des Applications', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Application')\n",
    "axes[1, 0].set_ylabel('Nombre d\\'√©chantillons')\n",
    "\n",
    "# Exemple de s√©quences\n",
    "axes[1, 1].plot(X_sequences[0], label='PIN exemple', alpha=0.7)\n",
    "axes[1, 1].plot(X_sequences[len(X_sequences)//2], label='App exemple', alpha=0.7)\n",
    "axes[1, 1].set_title('Exemples de S√©quences Normalis√©es', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Temps')\n",
    "axes[1, 1].set_ylabel('Voltage normalis√©')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0173ba-f92f-418a-980f-328918ec2b54",
   "metadata": {},
   "source": [
    "----------------\n",
    "Creation of Decision tree + SVM toanalyse activity\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b3e47-1f77-4cd8-a63b-c926861044e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     √âTAPE 1/5 : CR√âATION DU CLASSIFICATEUR DT + SVM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ActivityClassifier:\n",
    "    \"\"\"\n",
    "    Classificateur d'activit√© : Decision Tree + SVM en Voting\n",
    "    D√©termine si l'√©chantillon est un PIN (0) ou une Application (1)\n",
    "    Utilise 19 features extraites des signaux\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.dt_classifier = None\n",
    "        self.svm_classifier = None\n",
    "        self.voting_classifier = None\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def create(self):\n",
    "        \"\"\"Cr√©e le classificateur combin√©\"\"\"\n",
    "        print(\"\\nüî® Cr√©ation du classificateur...\")\n",
    "        \n",
    "        # Decision Tree\n",
    "        self.dt_classifier = DecisionTreeClassifier(\n",
    "            max_depth=10,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"   ‚úÖ Decision Tree cr√©√© (max_depth=10)\")\n",
    "        \n",
    "        # SVM avec kernel RBF\n",
    "        self.svm_classifier = SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"   ‚úÖ SVM cr√©√© (kernel=rbf)\")\n",
    "        \n",
    "        # Voting Classifier (soft voting)\n",
    "        self.voting_classifier = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('decision_tree', self.dt_classifier),\n",
    "                ('svm', self.svm_classifier)\n",
    "            ],\n",
    "            voting='soft'  # Utilise les probabilit√©s\n",
    "        )\n",
    "        print(\"   ‚úÖ Voting Classifier cr√©√© (soft voting)\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Cr√©er le classificateur\n",
    "activity_classifier = ActivityClassifier()\n",
    "activity_classifier.create()\n",
    "\n",
    "print(\"\\n‚úÖ Classificateur d'activit√© cr√©√© avec succ√®s!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0a659-cb8b-40d4-a70a-8b940dd7e304",
   "metadata": {},
   "source": [
    "----------------\n",
    "Training of DT + SVM\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36bf7c-a336-4e6e-b57f-70010f8bfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     √âTAPE 2/5 : ENTRA√éNEMENT DU DT + SVM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def train_activity_classifier(classifier, X_features, y_activity, test_size=0.25):\n",
    "    \"\"\"\n",
    "    Entra√Æne le classificateur d'activit√©\n",
    "    Utilise 75% pour l'entra√Ænement, 25% pour le test (selon le papier)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Pr√©paration des donn√©es...\")\n",
    "    \n",
    "    # Split train/test (75/25 selon le papier)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y_activity, \n",
    "        test_size=test_size, \n",
    "        random_state=42, \n",
    "        stratify=y_activity\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train: {X_train.shape[0]} √©chantillons (75%)\")\n",
    "    print(f\"   Test:  {X_test.shape[0]} √©chantillons (25%)\")\n",
    "    \n",
    "    # Normalisation des features (important pour SVM)\n",
    "    print(\"\\nüîÑ Normalisation des 19 features...\")\n",
    "    X_train_scaled = classifier.scaler.fit_transform(X_train)\n",
    "    X_test_scaled = classifier.scaler.transform(X_test)\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    print(\"\\nüöÄ Entra√Ænement en cours...\")\n",
    "    classifier.voting_classifier.fit(X_train_scaled, y_train)\n",
    "    classifier.is_trained = True\n",
    "    \n",
    "    # √âvaluation\n",
    "    print(\"\\nüìà √âvaluation...\")\n",
    "    train_score = classifier.voting_classifier.score(X_train_scaled, y_train)\n",
    "    test_score = classifier.voting_classifier.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print(f\"\\n‚úÖ R√©sultats:\")\n",
    "    print(f\"   Train Accuracy: {train_score:.4f} ({train_score*100:.2f}%)\")\n",
    "    print(f\"   Test Accuracy:  {test_score:.4f} ({test_score*100:.2f}%)\")\n",
    "    \n",
    "    # Pr√©dictions d√©taill√©es\n",
    "    y_pred = classifier.voting_classifier.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"\\nüìä Rapport de classification:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                               target_names=['PIN', 'Application'],\n",
    "                               digits=4))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['PIN', 'Application'],\n",
    "                yticklabels=['PIN', 'Application'],\n",
    "                cbar_kws={'label': 'Nombre d\\'√©chantillons'})\n",
    "    plt.title('Matrice de Confusion - DT+SVM (Activit√©)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('Vraie Activit√©')\n",
    "    plt.xlabel('Activit√© Pr√©dite')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix_activity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_score, test_score\n",
    "\n",
    "# Entra√Æner\n",
    "train_acc, test_acc = train_activity_classifier(\n",
    "    activity_classifier, \n",
    "    X_features, \n",
    "    y_activity,\n",
    "    test_size=0.25  # 75/25 split selon le papier\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ DT + SVM entra√Æn√© avec succ√®s!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf37da-4a29-4f4e-a780-5b1de7339d59",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "Definition of modele\n",
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a466e-b241-42b6-8753-03f872215fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e1c93-a009-4cc4-ba90-61acb2722adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d5e7d-7fca-41ce-bf83-629682fac515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3227ae-2a19-4e39-8924-be66a1d68587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37bb2a-a147-43d9-bc59-06bf596d6dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a655182-c461-4147-bad2-6064ef4ac25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46daa42-fa54-47b3-bd4d-9a95e98d31f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6cf11-51a8-464b-badf-9ba792674f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4d5be-ac87-472d-9a51-60fba4f3cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af969ce5-7491-4a11-a876-78fd6665daed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b4d7e-0478-4fd9-813a-d27f7541bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d231ad7-7265-4b36-81c2-6dabc0fe730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bff13ba-127a-46b6-abe3-08eb9832d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonction create_cnn_model d√©finie\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Cr√©e le mod√®le CNN 1D selon l'architecture du sch√©ma:\n",
    "    - C-16 KS-8 S-4: Conv1D 16 filtres, kernel_size=8, stride=4\n",
    "    - C-32 KS-8 S-4: Conv1D 32 filtres, kernel_size=8, stride=4\n",
    "    - C-64 KS-8 S-4: Conv1D 64 filtres, kernel_size=8, stride=4\n",
    "    - Chaque conv suivie de BatchNorm + ReLU\n",
    "    - Flatten + Dense + Output\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape, name='input'),\n",
    "        \n",
    "        # Bloc 1: C-16 KS-8 S-4\n",
    "        layers.Conv1D(filters=16, kernel_size=8, strides=4, padding='same', name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.ReLU(name='relu1'),\n",
    "        \n",
    "        # Bloc 2: C-32 KS-8 S-4\n",
    "        layers.Conv1D(filters=32, kernel_size=8, strides=4, padding='same', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.ReLU(name='relu2'),\n",
    "        \n",
    "        # Bloc 3: C-64 KS-8 S-4\n",
    "        layers.Conv1D(filters=64, kernel_size=8, strides=4, padding='same', name='conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.ReLU(name='relu3'),\n",
    "        \n",
    "        # Flatten\n",
    "        layers.Flatten(name='flatten'),\n",
    "        \n",
    "        # First fully-connected layer\n",
    "        layers.Dense(128, name='fc1'),\n",
    "        layers.Dropout(0.5, name='dropout1'),\n",
    "        \n",
    "        # Sigmoid activation pour augmenter la non-lin√©arit√© (selon le papier)\n",
    "        layers.Activation('sigmoid', name='sigmoid'),\n",
    "        \n",
    "        # Second fully-connected layer\n",
    "        layers.Dense(64, name='fc2'),\n",
    "        layers.Dropout(0.3, name='dropout2'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='CNN_Keystroke_Model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Fonction create_cnn_model d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5644eb-90b4-47d4-aa6b-33fc962c3a3d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "Utils Functions\n",
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a568864-ad7f-4059-a82b-abd1cc3fba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires d√©finies\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(X, y, test_size=0.2, val_size=0.1, normalize=True):\n",
    "    \"\"\"\n",
    "    Pr√©pare les donn√©es pour l'entra√Ænement.\n",
    "    \n",
    "    Args:\n",
    "        X: array (n_samples, sequence_length) - s√©quences de volts\n",
    "        y: array (n_samples,) - labels (digits)\n",
    "        test_size: proportion du test set\n",
    "        val_size: proportion du validation set (par rapport au train)\n",
    "        normalize: normaliser les s√©quences\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Pr√©paration des donn√©es\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Normalisation (Z-score normalization)       ---------------------------------------------------------------------\n",
    "    if normalize:\n",
    "        print(\"üìä Normalisation des donn√©es...\")\n",
    "        mean = np.mean(X, axis=1, keepdims=True)\n",
    "        std = np.std(X, axis=1, keepdims=True) + 1e-8\n",
    "        X = (X - mean) / std\n",
    "    \n",
    "    # Reshape pour Conv1D: (n_samples, sequence_length, 1)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    print(f\"üìê Shape apr√®s reshape: {X.shape}\")\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"‚úÖ Val set:   {X_val.shape[0]} samples\")\n",
    "    print(f\"‚úÖ Test set:  {X_test.shape[0]} samples\")\n",
    "    print(f\"‚úÖ Classes:   {len(np.unique(y))} ({np.unique(y)})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"Visualise l'historique d'entra√Ænement\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Graphique sauvegard√©: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Affiche la matrice de confusion\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Matrice de confusion sauvegard√©e: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfab59-eec2-4327-ae92-179adb35ddbe",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "Charge Data\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0779783c-0456-474a-b188-262f998c862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Charge les donn√©es depuis un fichier CSV\n",
    "    Format attendu: premi√®re colonne = digit, colonnes suivantes = valeurs de volt\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Chargement depuis: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extraire les labels (premi√®re colonne)\n",
    "    y = df.iloc[:, 0].values\n",
    "    \n",
    "    # Extraire les features (colonnes restantes)\n",
    "    X = df.iloc[:, 1:].values\n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es charg√©es: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"   Classes trouv√©es: {np.unique(y)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# # Charger vos donn√©es\n",
    "# X, y = load_data_from_csv('/mnt/c/Users/VotreNom/Documents/votre_fichier.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba4c4f-3217-40f9-834e-c9cf96a1d7a7",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "Data exploration\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b808d-57f2-4244-baf0-c0e50f615d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Exploration des donn√©es\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")\n",
    "print(f\"Type de X: {X.dtype}\")\n",
    "print(f\"Type de y: {y.dtype}\")\n",
    "print(f\"\\nNombre de classes: {len(np.unique(y))}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"\\nDistribution des classes:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Classe {cls}: {count} samples ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nStatistiques des s√©quences:\")\n",
    "print(f\"  Min: {X.min():.4f}\")\n",
    "print(f\"  Max: {X.max():.4f}\")\n",
    "print(f\"  Mean: {X.mean():.4f}\")\n",
    "print(f\"  Std: {X.std():.4f}\")\n",
    "\n",
    "# Visualiser quelques s√©quences\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(6):\n",
    "    idx = np.random.randint(0, len(X))\n",
    "    axes[i].plot(X[idx], linewidth=0.5)\n",
    "    axes[i].set_title(f'Sample {idx} - Classe {y[idx]}')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('Voltage')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_sequences.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5777b-22c3-456f-bd02-a9ef12fa23bc",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "Data preparation\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3d300-5623-4623-9ff8-6a7998ac01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les donn√©es\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    val_size=0.1,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# V√©rifier les shapes\n",
    "print(f\"\\nüìê Shapes finales:\")\n",
    "print(f\"   X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "print(f\"   X_val:   {X_val.shape}   | y_val:   {y_val.shape}\")\n",
    "print(f\"   X_test:  {X_test.shape}  | y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662fdb6-7fe8-4466-99e1-9c047ce31972",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "Creation and compilation of the CNN model\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a47ab4-9d84-4f0a-9b7f-61c20f8d70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres du mod√®le\n",
    "input_shape = (X_train.shape[1], 1)  # (sequence_length, 1)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cr√©ation du mod√®le\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Nombre de classes: {num_classes}\")\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le compil√© et pr√™t pour l'entra√Ænement\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056a4b6-8253-4c30-9956-b48d0a369966",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "Callback Configuration\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e588d-d6eb-428c-a356-1f0a152f026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un timestamp pour les fichiers\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"keystroke_cnn_{timestamp}\"\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early Stopping: arr√™te si pas d'am√©lioration\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce Learning Rate: r√©duit le LR si plateau\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model Checkpoint: sauvegarde le meilleur mod√®le\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_name}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard: pour visualiser l'entra√Ænement\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=f'./logs/{model_name}',\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Callback personnalis√© pour surveiller l'overfitting en temps r√©el\n",
    "class OverfittingMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"Surveille l'overfitting pendant l'entra√Ænement\"\"\"\n",
    "    def __init__(self, patience=3):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        train_acc = logs.get('accuracy')\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        \n",
    "        # Calculer les √©carts\n",
    "        loss_gap = abs(val_loss - train_loss)\n",
    "        acc_gap = abs(train_acc - val_acc)\n",
    "        \n",
    "        # V√©rifier l'am√©lioration\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        # Alertes d'overfitting\n",
    "        if loss_gap > 0.3 or acc_gap > 0.1:\n",
    "            print(f\"\\n  ‚ùå ALERTE: Overfitting s√©v√®re √† l'epoch {epoch+1}!\")\n",
    "            print(f\"     Loss gap: {loss_gap:.4f} | Acc gap: {acc_gap:.4f}\")\n",
    "        elif loss_gap > 0.15 or acc_gap > 0.05:\n",
    "            print(f\"\\n  ‚ö†Ô∏è  Overfitting mod√©r√© √† l'epoch {epoch+1}\")\n",
    "            print(f\"     Loss gap: {loss_gap:.4f} | Acc gap: {acc_gap:.4f}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Callbacks configur√©s\")\n",
    "print(f\"üìÅ Mod√®le sera sauvegard√©: {model_name}_best.keras\")\n",
    "print(f\"üìä Logs TensorBoard: ./logs/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4dba0-83f4-4417-b61a-5597bb4c1c40",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "Model Training\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829723c-c1dd-47d9-8e72-fd0bbf6c5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hyperparam√®tres\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs max: {EPOCHS}\")\n",
    "print(\"\\nüöÄ Entra√Ænement en cours...\")\n",
    "\n",
    "# Entra√Ænement\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f5f06-6b45-4f66-85cd-b223b9001655",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "Final Overfitting Analysis\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f26b1-61d9-45ac-ba97-5246528614b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_overfitting(history):\n",
    "    \"\"\"D√©tecte et analyse l'overfitting\"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    # M√©triques finales\n",
    "    final_train_loss = train_loss[-1]\n",
    "    final_val_loss = val_loss[-1]\n",
    "    final_train_acc = train_acc[-1]\n",
    "    final_val_acc = val_acc[-1]\n",
    "    \n",
    "    loss_gap = abs(final_val_loss - final_train_loss)\n",
    "    acc_gap = abs(final_train_acc - final_val_acc)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç ANALYSE DE L'OVERFITTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä M√©triques finales:\")\n",
    "    print(f\"   Train Loss:     {final_train_loss:.4f}\")\n",
    "    print(f\"   Val Loss:       {final_val_loss:.4f}\")\n",
    "    print(f\"   √âcart Loss:     {loss_gap:.4f}\")\n",
    "    print(f\"   Train Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "    print(f\"   Val Accuracy:   {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "    print(f\"   √âcart Accuracy: {acc_gap:.4f} ({acc_gap*100:.2f}%)\")\n",
    "    \n",
    "    # Diagnostic\n",
    "    print(f\"\\nüéØ Diagnostic:\")\n",
    "    if loss_gap > 0.3 or acc_gap > 0.1:\n",
    "        print(\"   ‚ùå OVERFITTING S√âV√àRE d√©tect√© !\")\n",
    "        print(f\"\\nüí° Recommandations:\")\n",
    "        print(\"   ‚Üí Augmenter le Dropout √† 0.6-0.7\")\n",
    "        print(\"   ‚Üí R√©duire la complexit√© du mod√®le\")\n",
    "        print(\"   ‚Üí Ajouter plus de donn√©es d'entra√Ænement\")\n",
    "        print(\"   ‚Üí Utiliser de la r√©gularisation L2\")\n",
    "    elif loss_gap > 0.15 or acc_gap > 0.05:\n",
    "        print(\"   ‚ö†Ô∏è  OVERFITTING MOD√âR√â d√©tect√©\")\n",
    "        print(f\"\\nüí° Recommandations:\")\n",
    "        print(\"   ‚Üí Augmenter l√©g√®rement le Dropout\")\n",
    "        print(\"   ‚Üí Utiliser Early Stopping plus agressif (patience=5)\")\n",
    "        print(\"   ‚Üí Ajouter du Data Augmentation\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ PAS D'OVERFITTING - Le mod√®le g√©n√©ralise bien !\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return loss_gap, acc_gap\n",
    "\n",
    "# Analyser l'overfitting\n",
    "loss_gap, acc_gap = detect_overfitting(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bff11-f036-472e-8605-eed75d515eb7",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "Detailled Overfitting Visualisation\n",
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92cf67-880a-4d3c-91e2-f6e1f6abcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overfitting_analysis(history, save_path='overfitting_analysis.png'):\n",
    "    \"\"\"Cr√©e une visualisation compl√®te pour analyser l'overfitting\"\"\"\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # Calculer les √©carts\n",
    "    loss_gaps = [abs(v - t) for v, t in zip(val_loss, train_loss)]\n",
    "    acc_gaps = [abs(v - t) for v, t in zip(val_acc, train_acc)]\n",
    "    \n",
    "    # 1. Loss Comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, train_loss, 'b-', linewidth=2, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', linewidth=2, label='Val Loss')\n",
    "    plt.title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy Comparison\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, train_acc, 'b-', linewidth=2, label='Train Acc')\n",
    "    plt.plot(epochs, val_acc, 'r-', linewidth=2, label='Val Acc')\n",
    "    plt.title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Loss Gap\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, loss_gaps, 'purple', linewidth=2)\n",
    "    plt.axhline(y=0.15, color='orange', linestyle='--', label='Moderate')\n",
    "    plt.axhline(y=0.3, color='red', linestyle='--', label='Severe')\n",
    "    plt.title('Loss Gap (|Val - Train|)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gap')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Accuracy Gap\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, acc_gaps, 'green', linewidth=2)\n",
    "    plt.axhline(y=0.05, color='orange', linestyle='--', label='Moderate')\n",
    "    plt.axhline(y=0.1, color='red', linestyle='--', label='Severe')\n",
    "    plt.title('Accuracy Gap (|Val - Train|)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gap')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Val/Train Loss Ratio\n",
    "    plt.subplot(2, 3, 5)\n",
    "    loss_ratios = [v/t if t > 0 else 1 for v, t in zip(val_loss, train_loss)]\n",
    "    plt.plot(epochs, loss_ratios, 'brown', linewidth=2)\n",
    "    plt.axhline(y=1.0, color='green', linestyle='--', label='Perfect')\n",
    "    plt.axhline(y=1.2, color='orange', linestyle='--', label='Moderate')\n",
    "    plt.axhline(y=1.5, color='red', linestyle='--', label='Severe')\n",
    "    plt.title('Val/Train Loss Ratio', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Diagnostic Final\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    final_loss_gap = loss_gaps[-1]\n",
    "    final_acc_gap = acc_gaps[-1]\n",
    "    \n",
    "    if final_loss_gap > 0.3 or final_acc_gap > 0.1:\n",
    "        status = \"‚ùå OVERFITTING\\nS√âV√àRE\"\n",
    "        color = 'red'\n",
    "    elif final_loss_gap > 0.15 or final_acc_gap > 0.05:\n",
    "        status = \"‚ö†Ô∏è  OVERFITTING\\nMOD√âR√â\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        status = \"‚úÖ BON\\nAJUSTEMENT\"\n",
    "        color = 'green'\n",
    "    \n",
    "    text = f\"\"\"\n",
    "DIAGNOSTIC FINAL\n",
    "\n",
    "{status}\n",
    "\n",
    "Train Loss: {train_loss[-1]:.4f}\n",
    "Val Loss:   {val_loss[-1]:.4f}\n",
    "Loss Gap:   {final_loss_gap:.4f}\n",
    "\n",
    "Train Acc:  {train_acc[-1]:.4f}\n",
    "Val Acc:    {val_acc[-1]:.4f}\n",
    "Acc Gap:    {final_acc_gap:.4f}\n",
    "\n",
    "Epochs: {len(train_loss)}\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.text(0.1, 0.5, text, fontsize=11, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor=color, alpha=0.3),\n",
    "             family='monospace')\n",
    "    \n",
    "    plt.suptitle('üîç Analyse Compl√®te de l\\'Overfitting', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Analyse sauvegard√©e: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Cr√©er la visualisation compl√®te\n",
    "plot_overfitting_analysis(history, save_path=f'{model_name}_overfitting.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f1bf7-178d-40b9-8f0f-3bcca4c109e3",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "Standard Training Visualisation\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374129e-6121-4573-b276-69448aaf1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'historique d'entra√Ænement\n",
    "plot_training_history(history, save_path=f'{model_name}_history.png')\n",
    "\n",
    "# Afficher les m√©triques finales\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"M√©triques d'entra√Ænement\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Meilleure val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Meilleure val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Epoch final: {len(history.history['loss'])}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e412e5-d227-4c7c-b580-1bcaf7a6d284",
   "metadata": {},
   "source": [
    "--------------\n",
    "Evaluation on test set\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840002d6-d3ef-472d-be2c-fbb29f45b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"√âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# √âvaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats sur le test set:\")\n",
    "print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# Pr√©dictions\n",
    "print(\"\\nüîÆ G√©n√©ration des pr√©dictions...\")\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=[str(i) for i in range(num_classes)]))\n",
    "\n",
    "# Matrice de confusion\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred, \n",
    "    classes=list(range(num_classes)),\n",
    "    save_path=f'{model_name}_confusion_matrix.png'\n",
    ")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69b55c-494f-45f1-aa7d-d1c096108dbd",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "Saving of final model\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42535eee-8d08-4bfb-8c81-a2ebfaa29f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = f'{model_name}_final.keras'\n",
    "model.save(final_model_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAUVEGARDE DU MOD√àLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Mod√®le final sauvegard√©: {final_model_path}\")\n",
    "print(f\"‚úÖ Meilleur mod√®le sauvegard√©: {model_name}_best.keras\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pour charger le mod√®le plus tard:\n",
    "print(\"\\nPour charger le mod√®le:\")\n",
    "print(f\"  model = keras.models.load_model('{final_model_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1415a-3138-44c8-add6-8a0fedd78448",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "Test on testing samples\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f54ca-b85d-4ff3-9afc-97bed3712eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de pr√©diction sur quelques √©chantillons\n",
    "n_samples = 10\n",
    "sample_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"PR√âDICTIONS SUR {n_samples} √âCHANTILLONS AL√âATOIRES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    sample = X_test[idx:idx+1]  # Shape: (1, sequence_length, 1)\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Pr√©diction\n",
    "    pred_proba = model.predict(sample, verbose=0)[0]\n",
    "    pred_label = np.argmax(pred_proba)\n",
    "    confidence = pred_proba[pred_label]\n",
    "    \n",
    "    # Afficher\n",
    "    status = \"‚úÖ\" if pred_label == true_label else \"‚ùå\"\n",
    "    print(f\"\\n{status} √âchantillon {i+1}:\")\n",
    "    print(f\"   Vraie classe:   {true_label}\")\n",
    "    print(f\"   Classe pr√©dite: {pred_label}\")\n",
    "    print(f\"   Confiance:      {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    \n",
    "    # Top 3 pr√©dictions\n",
    "    top3_idx = np.argsort(pred_proba)[-3:][::-1]\n",
    "    print(f\"   Top 3: {[(idx, f'{pred_proba[idx]:.3f}') for idx in top3_idx]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe697394-1519-41cf-94a6-c00bb1852969",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "Final resume and Instructions\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4487cb0-6eae-497b-8ca9-4b2d6a82f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    üéâ R√âSUM√â FINAL üéâ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s!\")\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"\\nüìÅ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   - {model_name}_best.keras (meilleur mod√®le)\")\n",
    "print(f\"   - {model_name}_final.keras (mod√®le final)\")\n",
    "print(f\"   - {model_name}_history.png (courbes d'apprentissage)\")\n",
    "print(f\"   - {model_name}_confusion_matrix.png (matrice de confusion)\")\n",
    "print(f\"   - ./logs/{model_name}/ (logs TensorBoard)\")\n",
    "\n",
    "print(f\"\\nüìä Pour visualiser avec TensorBoard:\")\n",
    "print(f\"   tensorboard --logdir=./logs/{model_name}\")\n",
    "print(f\"   Puis ouvrir: http://localhost:6006\")\n",
    "\n",
    "print(f\"\\nüîÆ Pour utiliser le mod√®le plus tard:\")\n",
    "print(f\"   from tensorflow import keras\")\n",
    "print(f\"   model = keras.models.load_model('{model_name}_best.keras')\")\n",
    "print(f\"   predictions = model.predict(new_data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
